%%writefile predict.py
"""
Inference script to generate predictions for the test set.
Computes aggregate features on all train data (has_read=1) and applies them to test set,
then generates predictions using the trained model.
"""
import lightgbm as lgb
import numpy as np
import pandas as pd
import config  
import constants 
from features import add_aggregate_features, handle_missing_values 

def predict() -> None:
    """Generates and saves predictions for the test set.
    This script loads prepared data from data/processed/, computes aggregate features
    on all train data (has_read=1), applies them to test set, and generates predictions using
    the trained model.
    Note: Data must be prepared first using prepare_data.py, and model must be trained
    using train.py
    """
    # Load prepared data
    processed_path = config.PROCESSED_DATA_DIR / constants.PROCESSED_DATA_FILENAME
    if not processed_path.exists():
        raise FileNotFoundError(
            f"Processed data not found at {processed_path}. "
            "Please run 'python prepare_data.py' first."
        )
    print(f"Loading prepared data from {processed_path}...")
    featured_df = pd.read_parquet(processed_path, engine="pyarrow")
    print(f"Loaded {len(featured_df):,} rows with {len(featured_df.columns)} features")

    # Separate train and test sets
    # Use only rows with ratings (has_read=1) for aggregate calculations context
    train_set_for_aggregates = featured_df[(featured_df[constants.COL_SOURCE] == constants.VAL_SOURCE_TRAIN) & (featured_df[constants.COL_HAS_READ] == 1)].copy()
    test_set = featured_df[featured_df[constants.COL_SOURCE] == constants.VAL_SOURCE_TEST].copy()
    print(f"Train set used for aggregate calculations (has_read=1): {len(train_set_for_aggregates):,} rows")
    print(f"Test set: {len(test_set):,} rows")

    # Compute aggregate features on ALL train data (has_read=1) (to use for test predictions)
    # Use train_set_for_aggregates (has_read=1) for target-dependent aggregates
    print("\nComputing aggregate features on all train data (has_read=1)...")
    test_set_with_agg = add_aggregate_features(test_set.copy(), train_set_for_aggregates)

    # Handle missing values (use train_set_for_aggregates (has_read=1) for fill values)
    print("Handling missing values...")
    test_set_final = handle_missing_values(test_set_with_agg, train_set_for_aggregates)

    # Define features (exclude source, target, prediction, timestamp columns)
    exclude_cols = [
        constants.COL_SOURCE,
        config.TARGET, # Should not exist in test set, but exclude just in case
        constants.COL_PREDICTION,
        constants.COL_TIMESTAMP,
    ]
    features = [col for col in test_set_final.columns if col not in exclude_cols]
    # Exclude any remaining object columns that are not model features
    non_feature_object_cols = test_set_final[features].select_dtypes(include=["object"]).columns.tolist()
    features = [f for f in features if f not in non_feature_object_cols]

    X_test = test_set_final[features]
    print(f"Prediction features: {len(features)}")

    # Load trained model
    model_path = config.MODEL_DIR / config.MODEL_FILENAME
    if not model_path.exists():
        raise FileNotFoundError(
            f"Model not found at {model_path}. " "Please run 'python train.py' first."
        )
    print(f"\nLoading model from {model_path}...")
    model = lgb.Booster(model_file=str(model_path))

    # Generate predictions
    print("Generating predictions...")
    test_preds = model.predict(X_test)

    # Clip predictions to be within the valid rating range [0, 10]
    clipped_preds = np.clip(test_preds, constants.PREDICTION_MIN_VALUE, constants.PREDICTION_MAX_VALUE)

    # Create submission file
    submission_df = test_set[[constants.COL_USER_ID, constants.COL_BOOK_ID]].copy()
    submission_df[constants.COL_PREDICTION] = clipped_preds

    # Ensure submission directory exists
    config.SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)
    submission_path = config.SUBMISSION_DIR / constants.SUBMISSION_FILENAME
    submission_df.to_csv(submission_path, index=False)
    print(f"\nSubmission file created at: {submission_path}")
    print(f"Predictions: min={clipped_preds.min():.4f}, max={clipped_preds.max():.4f}, mean={clipped_preds.mean():.4f}")

if __name__ == "__main__":
    predict()
